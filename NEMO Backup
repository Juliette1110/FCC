# -*- coding: utf-8 -*-
#"""Nemo backup.ipynb

#Automatically generated by Colaboratory.

#Original file is located at
#    https://colab.research.google.com/drive/1mft39TK2HzgjBw7Nakf2ylrZcPBb13PE
#"""

import imaplib
import pickle
import random
import email
import spacy
import nltk
import gensim
from gensim import corpora
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import wordnet as wn
from spacy.lang.en import English

nltk.download('stopwords')
nltk.download('wordnet')
spacy.load('en_core_web_sm')



#Relevent Functions:

def tokenize(text):
    lda_tokens = []
    tokens = parser(text)
    for token in tokens:
        if token.orth_.isspace():
            continue
        elif token.like_url:
            lda_tokens.append('URL')
        elif token.orth_.startswith('@'):
            lda_tokens.append('SCREEN_NAME')
        else:
            lda_tokens.append(token.lower_)
    return lda_tokens

def get_lemma(word):
    lemma = wn.morphy(word)
    if lemma is None:
        return word
    else:
        return lemma
      
def get_lemma2(word):
    return WordNetLemmatizer().lemmatize(word)

def prepare_text_for_lda(text):
    tokens = tokenize(text)
    tokens = [token for token in tokens if len(token) > 4]
    tokens = [token for token in tokens if token not in en_stop]
    tokens = [get_lemma(token) for token in tokens]
    return tokens



#Log into email
ORG_EMAIL   = "@nd.edu"
FROM_EMAIL  = "jburcham" + ORG_EMAIL
FROM_PWD    = password = input("please enter password:") #logging on
SMTP_SERVER = "imap.gmail.com" #import gmail
SMTP_PORT   = 993
#establish cennection to email server
mail = imaplib.IMAP4_SSL(SMTP_SERVER)
mail.login(FROM_EMAIL,FROM_PWD)



"""The next task is to download all of the emails and split the subject and message text into 'topic' using Gensium. This is known as topic modeling. So download the messages/subjects:"""

mail.select('inbox')
#now fetch email ids
type, data = mail.search(None, 'ALL')
mail_ids = data[0]
id_list = mail_ids.split()
#fetch subject and headers
first_email_id = int(id_list[0])
latest_email_id = int(id_list[-1])
print(first_email_id)

#typ, data = mail.fetch('1', '(RFC822)' )
print("ONE")

hat=[]
for i in range(latest_email_id,latest_email_id-100, -1):
	typ, data = mail.fetch(str(i), '(RFC822)' )
	#print("TWO")
	for response_part in data:
		if isinstance(response_part, tuple):
                    msg = email.message_from_string(response_part[1].decode("utf-8",'ignore') )
                    for part in msg.walk():
                      if part.get_content_type() == 'text/plain':
                        #print (part.get_payload())
                        hat.append(part.get_payload())

tree=" ".join(hat)
test=tree

print("ORANGE")



"""Now use the gensium code. Source: susanli2016 https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/topic_modeling_Gensim.ipynb"""

parser = English()
en_stop = set(nltk.corpus.stopwords.words('english'))  
text_data=[]  
for word in test:
	tokens= prepare_text_for_lda(test)
	#print("PREP")
	text_data.append(tokens)
	#print("DPREDP")



"""Now to split the message data into topics. Lets start with 5 and see if a definite "food" one appears"""
print("blue")

dictionary = corpora.Dictionary(text_data)
dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)
corpus = [dictionary.doc2bow(text) for text in text_data]

pickle.dump(corpus, open('corpus.pkl', 'wb'))
dictionary.save('dictionary.gensim')

print("Yellow")

NUM_TOPICS = 15
ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)
ldamodel.save('model5.gensim')

topics = ldamodel.print_topics(num_words=8)
for topic in topics:
    print(topic)



#To test the code I will have my program pull random emails from my inbox (except for the first 100 used to create the topic sets) and print the first 3 that it finds that are classified as the "food" topic
foodemail=[]
n=100
testlist=id_list
del testlist[-n:]
while len(foodemail)< 3:
	testID=random.choice(testlist)
	typ, data = mail.fetch(str(testID), '(RFC822)' )
	for response_part in data:
		if isinstance(response_part, tuple):
                    msg = email.message_from_string(response_part[1].decode("utf-8",'ignore'))
                    for part in msg.walk():
                      if part.get_content_type() == 'text/plain':
                        emailbody=part
			vector= dictionary.doc2bow(prepare_text_for_lda(mailbody))
			for index, score in sorted(ldamodel[vector], key=lambda tup: -1*tup[1]):
    				print("Score: {}\t Topic: {}".format(score, ldamodel.print_topic(index, 5)))
				if emailbody.target== #TBD
				foodemail.append(testID)
for i in foodemail:
typ, data = mail.fetch(str(i), '(RFC822)' )
	for response_part in data:
		if isinstance(response_part, tuple):
                    msg = email.message_from_string(response_part[1].decode("utf-8",'ignore') )
                    for part in msg.walk():
                      if part.get_content_type() == 'text/plain':
                        print (part.get_payload())
                        hat.append(part.get_payload())
print(foodemail)
